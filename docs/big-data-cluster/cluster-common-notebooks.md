---
title: Стандартный сценарий использования кластеров больших данных (BDC) с помощью записных книжек Jupyter и Azure Data Studio
titleSuffix: SQL Server Big Data Clusters
description: Стандартный сценарий использования кластеров больших данных (BDC) с помощью записных книжек Jupyter и Azure Data Studio в кластере больших данных SQL Server 2019.
author: cloudmelon
ms.author: melqin
ms.reviewer: mikeray
ms.metadata: seo-lt-2019
ms.date: 09/22/2020
ms.topic: conceptual
ms.prod: sql
ms.technology: big-data-cluster
ms.openlocfilehash: 99e62be597e4ce08d38db199116f1bd4d5ab33f6
ms.sourcegitcommit: 29a2be59c56f8a4b630af47760ef38d2bf56a3eb
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/22/2020
ms.locfileid: "92378485"
---
# <a name="common-notebooks-for-sql-server-big-data-clusters"></a>Распространенные записные книжки для кластеров больших данных SQL Server

В этой статье рассматриваются записные книжки для кластеров больших данных SQL Server. Исполняемые записные книжки (IPYNB) предназначены для SQL Server 2019. Они помогают отобразить общие сценарии кластеров больших данных.

Каждая записная книжка предназначена для проверки собственных зависимостей. Команда **Запуск всех ячеек** может завершиться успешно или вызвать исключение с *указанием* гиперссылки на другую записную книжку для решения проблемы с отсутствующей зависимостью. Следуйте указаниям гиперссылки на следующую записную книжку, нажмите **Запустить все ячейки** и после успешного возврата в исходную записную книжку снова нажмите **Запустить все ячейки** .

Если все зависимости установлены, но команда **Запустить все ячейки** завершается сбоем, каждая записная книжка проанализирует результаты и по возможности выдаст подсказку с гиперссылкой на другую записную книжку, чтобы дополнительно помочь в устранении проблемы.

## <a name="gathering-logs-from-big-data-cluster-bdc"></a>Сбор журналов из кластера больших данных (BDC)

Записные книжки в этом разделе используются в качестве необходимых компонентов для других записных книжек, например для входа в кластер и выхода из него.

|Имя |Описание |
|---|---|
|SOP005 — az login|Использование интерфейса командной строки az для входа в Azure. |
|SOP006 — az logout|Использование интерфейса командной строки az для выхода из Azure.|
|SOP007 — сведения о версии (azdata, кластер больших данных, Kubernetes)|Определение вспомогательных функций, используемых в записной книжке, в сведениях об управлении версиями.|
|SOP011 — указание контекста конфигурации Kubernetes|Установка конфигурации Kubernetes для использования. |
|SOP028 — azdata login|Использование интерфейса командной строки azdata для входа в кластер больших данных. |
|SOP033 — azdata logout|Использование интерфейса командной строки azdata для выхода из кластера больших данных. |
|SOP034 — ожидание работоспособности BDC|Блокировка до тех пор, пока кластер больших данных не будет работоспособным или до истечения указанного времени ожидания. Параметр min_pod_count указывает, что проверка работоспособности не будет пройдена до тех пор, пока в кластере не будет такого числа модулей pod. Если какие-либо существующие модули pod, которые выходят за пределы этого ограничения, неработоспособны, кластер также будет неработоспособен.|
|OPR001 — создание app-deploy|Используйте эту записную книжку для создания приложения в кластере больших данных. |
|OPR002 — запуск app-deploy|Используйте эту записную книжку для запуска приложения в кластере больших данных. |
|OPR003 — создание cronjob|Используйте эту записную книжку для создания cronjob в кластере больших данных. |
|OPR004 — приостановка cronjob|Используйте эту записную книжку для приостановки cronjob в кластере больших данных. |
|OPR005 — возобновление cronjob|Используйте эту записную книжку для возобновления cronjob в кластере больших данных. |
|OPR006 — удаление cronjob|Используйте эту записную книжку для удаления cronjob в кластере больших данных. |
|OPR007 — удаление app-deploy|Используйте эту записную книжку для удаления приложения в кластере больших данных. |
|OPR100 — развертывание записных книжек и планирование их работы|Используйте эту записную книжку для развертывания списка записных книжек в модуле pod App-Deploy, запуска App-Deploy по расписанию с помощью Kubernetes CronJob и установки панели мониторинга Grafana для просмотра результатов.|
|OPR600 — мониторинг инфраструктуры (Kubernetes)|Используйте эту записную книжку для мониторинга инфраструктуры.|
|OPR700 — создание панели мониторинга Grafana для приложений App-Deploy|Используйте эту записную книжку для создания панели мониторинга в Grafana для мониторинга результатов App-Deploy и создания оповещений в случае сбоя предохранителя или приложения.|
|OPR900 — запуск app-deploy для устранения неполадок|Используйте эту записную книжку для запуска скрипта app-deploy непосредственно в контейнере (с помощью файла kubectl еxec). Это позволит получить дополнительные отладочные сведения для устранения неполадок, связанных с запуском скрипта.|
|OPR901 — устранение неполадок cronjob|Используйте эту записную книжку для запуска скрипта cronjob непосредственно в контейнере (с помощью файла kubectl еxec). Это позволит получить дополнительные отладочные сведения для устранения неполадок.|


## <a name="analyze-logs-from-big-data-clusters-bdc"></a>Анализ журналов из кластеров больших данных (BDC)

Набор записных книжек с примерами, демонстрирующими сценарии для кластера больших данных SQL Server.

|Имя |Описание |
|---|---|
|SAM001a — запрос пула носителей из главного пула SQL Server (1 из 3) — Загрузка примера данных|Используя сведения этого учебника, состоящего из 3 частей, загрузите данные в пул носителей (HDFS) с помощью azdata, преобразуйте их в формат Parquet (с помощью Spark). Запросите данные с помощью главного пула (SQL Server), используя сведения из третей части этого учебника. |
|SAM001b — запрос пула носителей из главного пула SQL Server (2 из 3) — преобразование данных в формат PARQUET|Используйте сведения из второй части учебника, состоящего из 3 частей, чтобы преобразовать CSV-файл в файл PARQUET с помощью Spark.|
|SAM001c — запрос пула носителей из главного пула SQL Server (3 из 3) — запрос HDFS из SQL Server|Из третьей части учебника, посвященного пулам носителей, вы узнаете, как создать внешнюю таблицу, указывающую на данные HDFS в кластере больших данных, а также как объединить эти данные с важными данными в главном экземпляре.|
|SAM002 — пул носителей (2 из 2) — запрос HDFS|Из второй части учебника, посвященного пулам носителей, вы узнаете, как создать внешнюю таблицу, указывающую на данные HDFS в кластере больших данных, а также как объединить эти данные с важными данными в главном экземпляре.|
|SAM003 — пример пула данных|Из этого учебника вы узнаете, как создать источник пула данных и внешнюю таблицу в пуле данных, а затем как добавить данные в таблицы пула данных и загрузить данные из одной таблицы пула данных в другую. Если объединить данные в таблице пула данных с другими таблицами пула данных, будет выполнено усечение и очистка таблицы. |
|SAM004 — виртуализация данных из MongoDB|Чтобы запросить данные из внешнего источника данных MongoDB, необходимо создать внешние таблицы, позволяющие ссылаться на внешние данные. Этот раздел содержит пример кода для создания таких внешних таблиц.|
|SAM005 — виртуализация данных из Oracle|Чтобы запросить данные из внешнего источника данных Oracle, необходимо создать внешние таблицы, позволяющие ссылаться на внешние данные. Этот раздел содержит пример кода для создания таких внешних таблиц.|
|SAM006 — виртуализация данных из SQL Server|Чтобы запросить данные практически из любого источника данных SQL Server, необходимо создать внешние таблицы, чтобы ссылаться на внешние данные. Этот раздел содержит пример кода для создания таких внешних таблиц.|
|SAM007 — виртуализация данных из Teradata|Чтобы запросить данные из внешнего источника данных Teradata, необходимо создать внешние таблицы, позволяющие ссылаться на внешние данные. Этот раздел содержит пример кода для создания таких внешних таблиц.|
|SAM008 — azdata в Spark|Команды azdata и kubectl для работы с сеансами Spark.|
|SAM009 — azdata в HDFS|Команды azdata и kubectl для работы с HDFS.|
|SAM010 — azdata в приложении|Команды azdata и kubectl для работы с App Deploy. |

## <a name="next-steps"></a>Дальнейшие действия

Дополнительные сведения о [!INCLUDE[big-data-clusters-2019](../includes/ssbigdataclusters-ss-nover.md)] см. в статье [Что такое [!INCLUDE[big-data-clusters-2019](../includes/ssbigdataclusters-ver15.md)]?](big-data-cluster-overview.md).
